## Drummiez AI Project: Process and Thinking

### 1. Understanding the Request

The user requested a Python-based backend project to:
- Parse musical drum sheets (PDF/image) using AI.
- Generate drum noises from the parsed notes.
- Provide API endpoints for frontend integration.
- Include a `README.md` and `.gitignore`.
- Document the development process.
- Set a default BPM of 100.

### 2. Initial Setup and Environment Management

- **Virtual Environment:** The first step was to set up a virtual environment to manage project dependencies. This ensures that project-specific libraries don't conflict with system-wide Python installations.
- **`pipenv`:** I initially attempted to use `pipenv` for dependency management due to its robust features (Pipfile, Pipfile.lock). However, encountering the `externally-managed-environment` error on macOS (a common issue with Homebrew Python) led to a slight detour.
- **`venv` and Manual `pipenv` Installation:** To overcome the `externally-managed-environment` issue, I opted to create a virtual environment using `python3 -m venv .venv` and then explicitly installed `pipenv` into that virtual environment using the full path to its `pip` executable (`.venv/bin/pip install pipenv`). This ensures `pipenv` operates within the isolated environment.

### 3. Core Project Structure and Initial Files

- **`main.py`:** This file will house the FastAPI application, defining the API endpoints and orchestrating the core logic.
- **`README.md`:** Essential for project documentation, providing an overview, setup instructions, API details, and future plans.
- **`.gitignore`:** Crucial for version control, preventing unnecessary files (like virtual environment folders, compiled Python files, OS-specific files) from being committed to the repository.
- **`process_explanation.txt`:** This file itself, detailing the thought process and decisions made during development.

### 4. API Design (FastAPI)

- **Framework Choice:** FastAPI was chosen for its modern features, high performance, automatic interactive API documentation (Swagger UI/ReDoc), and ease of use for building asynchronous APIs in Python.
- **Endpoints Defined:**
    - `GET /`: A simple root endpoint for a welcome message, useful for testing API availability.
    - `POST /parse_drumsheet/`: This endpoint will handle the upload of drum sheet files (image/PDF). It takes an `UploadFile` and an optional `bpm` parameter. The core AI parsing logic will reside here.
    - `POST /generate_drum_audio/`: This endpoint will receive the parsed notes (as a dictionary) and generate the corresponding drum audio. It also accepts an optional `bpm` parameter.
- **Default BPM:** The `bpm` parameter in both parsing and generation endpoints defaults to 100, as per the user's requirement.

### 5. Library Choices and Integration

- **Optical Music Recognition (OMR): `Oemer`**
    - Chosen as an end-to-end OMR system that outputs MusicXML, a standard format for musical notation.
    - While `Oemer` is general-purpose, its MusicXML output can be further processed to extract drum-specific information.
    - **Current Status:** Integrated as a placeholder in `main.py` with simulated MusicXML output. Actual integration will involve calling `oemer.run()` with the uploaded image/PDF.

- **MusicXML Parsing and MIDI Generation: `music21`**
    - A powerful library for computer-aided musicology, capable of parsing MusicXML and converting it to MIDI.
    - Essential for bridging the gap between `Oemer`'s MusicXML output and audio generation.
    - **Integration:** Used in `main.py` to parse the MusicXML content and construct a `music21` stream, from which MIDI files can be generated.

- **MIDI to Audio Conversion: `midi2audio` (with `FluidSynth`)**
    - Provides a Python interface to `FluidSynth`, a software synthesizer, to convert MIDI files to audio (e.g., WAV).
    - **External Dependency:** `FluidSynth` is a system-level application and needs to be installed separately. A soundfont (`.sf2` file) is also required for `FluidSynth` to produce sounds.
    - **Integration:** Used in `main.py` to convert the generated MIDI stream into a WAV audio file. The `SOUNDFONT_PATH` is configurable via an environment variable.

### 6. Pre-requisites for Running the Application

To run the Drummiez AI application, you need to install `FluidSynth` and obtain a soundfont:

- **Install FluidSynth:**
    - **macOS:**
      ```bash
      brew install fluidsynth
      ```
    - **Debian/Ubuntu:**
      ```bash
      sudo apt-get update
      sudo apt-get install fluidsynth
      ```
    - **Windows:** Download from [FluidSynth's official website](https://www.fluidsynth.org/download/).

- **Obtain a Soundfont:**
    `FluidSynth` requires a soundfont (`.sf2` file) to produce sounds. You can download a General MIDI soundfont, for example, `FluidR3_GM.sf2`.
    Place the soundfont file in a known location, and set the `SOUNDFONT_PATH` environment variable to its absolute path. For example:
    ```bash
    export SOUNDFONT_PATH="/path/to/your/soundfont.sf2"
    ```
    A common path on Linux is `/usr/share/sounds/sf2/FluidR3_GM.sf2`.

### 7. Future Steps and Challenges

- **Full `Oemer` Integration:** Replace the simulated MusicXML output with actual calls to `Oemer` to process uploaded images/PDFs. This will require understanding `Oemer`'s API and output format in detail.
- **Robust Drum Note Extraction from MusicXML:** Develop more sophisticated logic within `music21` to accurately identify and interpret drum-specific notation from the MusicXML output. This might involve mapping MusicXML percussion elements to standard MIDI drum notes.
- **Error Handling and Robustness:** Implement comprehensive error handling for file uploads, `Oemer` processing failures, `music21` parsing issues, and `midi2audio` conversion problems.
- **Scalability:** Consider strategies for handling potentially large files and computationally intensive OMR tasks, possibly involving asynchronous processing or worker queues.
- **User Feedback and Iteration:** Gather feedback on the accuracy of drum sheet parsing and the quality of generated audio to refine the models and mappings.

### 8. Dataset Selection

For the task of parsing drum sheets, a specialized dataset of drum sheet music images is ideal. However, a public dataset for this specific purpose was not readily available.

As a suitable alternative, a large-scale Optical Music Recognition (OMR) dataset has been identified:

- **DeepScoresV2:** This dataset contains a vast number of rendered musical scores with detailed annotations for various musical symbols. While it's a general-purpose music dataset, it is highly likely to contain percussion and drum notation that can be used to train and evaluate an OMR model for this project.
  - **Link:** [https://zenodo.org/record/4782213](https://zenodo.org/record/4782213)

The next steps will involve exploring this dataset to filter and extract relevant drum sheet music examples for model training.

This document will be updated as the project progresses and more specific technical decisions are made regarding AI models and audio libraries.

### 9. Progress Log

**2025-11-06:**

- **Project Analysis:** Reviewed `README.md` and `process_explanation.txt` to understand the current state of the project.
- **Dataset Research:** Searched for a suitable dataset for Optical Music Recognition (OMR) of drum sheets.
- **Dataset Identification:** Identified the "DeepScoresV2" dataset as a strong candidate for training an OMR model. While not specific to drums, it is a large-scale dataset that likely contains relevant examples.
- **Codebase Update:** 
    - Added a comment in `main.py` with a link to the DeepScoresV2 dataset.
    - Added section `8. Dataset Selection` to this document (`process_explanation.txt`) to formally record the dataset choice and link.

**2025-11-06 (Phase 1 Completion):**

- **OMR Integration:** Replaced the placeholder OMR logic with a full integration of the `oemer` library in the `parse_drumsheet` function. The application now processes uploaded image/PDF files to generate MusicXML.
- **Improved Drum Note Extraction:**
    - Researched and implemented a comprehensive `DRUM_MIDI_MAP` based on the General MIDI standard.
    - Created a `get_midi_pitch` helper function to intelligently map MusicXML note properties to MIDI pitches.
    - Updated the `parse_drumsheet` function to use the new mapping and produce a more accurate list of MIDI notes.
- **Refined Audio Generation:** Simplified the `generate_drum_audio` function to directly use the MIDI pitches from the parsed notes, removing redundant mapping logic.
