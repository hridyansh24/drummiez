# To-Do List for Drummiez AI (as of 2025-11-06)

This list outlines the next steps to move the Drummiez AI project from its current state to a deployable application.

## Phase 1: Core Functionality Implementation

This phase focuses on building the essential features of the application.

1.  **Integrate Optical Music Recognition (OMR):**
    *   [x] Replace the placeholder MusicXML in `main.py` with a real OMR library.
    *   [x] Integrate `oemer` or another OMR library to process uploaded image/PDF files.
    *   [x] Handle the output of the OMR library (e.g., MusicXML file).

2.  **Improve Drum Note Extraction:**
    *   [ ] Research the MusicXML standard for percussion notation.
    *   [ ] Enhance the `music21` logic in `main.py` to accurately parse drum notes from the MusicXML output.
    *   [ ] Create a robust mapping from MusicXML percussion notation to MIDI drum sounds.

3.  **Refine Audio Generation:**
    *   [ ] Expand the `drum_midi_map` in `main.py` to include a wider range of drum sounds.
    *   [ ] Ensure the generated audio accurately reflects the timing and dynamics from the parsed drum sheet.

## Phase 2: Model Training and Improvement (Optional but Recommended)

This phase is for improving the accuracy of the OMR by training a custom model.

1.  **Dataset Preparation:**
    *   [ ] Download the DeepScoresV2 dataset ([https://zenodo.org/record/4782213](https://zenodo.org/record/4782213)).
    *   [ ] Write scripts to explore the dataset and filter for images containing drum or percussion notation.
    *   [ ] Preprocess the filtered images and annotations into a format suitable for model training.

2.  **OMR Model Training:**
    *   [ ] Choose a deep learning framework (e.g., TensorFlow, PyTorch).
    *   [ ] Implement and train a custom OMR model on the prepared dataset.
    *   [ ] Evaluate the model's performance and iterate on its architecture and training process.
    *   [ ] Integrate the trained model into the FastAPI application.

## Phase 3: Production Readiness and Deployment

This phase focuses on making the application robust, scalable, and ready for deployment.

1.  **Add Robust Error Handling and Validation:**
    *   [ ] Implement comprehensive validation for file uploads (file types, size limits).
    *   [ ] Add more specific error handling for the OMR, `music21`, and `midi2audio` processing steps.

2.  **Implement Testing:**
    *   [ ] Write unit tests for individual functions (e.g., note extraction, MIDI mapping).
    *   [ ] Write integration tests for the API endpoints.

3.  **Improve Configuration:**
    *   [ ] Move all configurable values (e.g., soundfont path, model paths) to environment variables or a configuration file.
    *   [ ] Use a library like Pydantic's `BaseSettings` for configuration management.

4.  **Containerize the Application:**
    *   [ ] Create a `Dockerfile` for the application.
    *   [ ] Ensure that all system dependencies, like `FluidSynth`, are installed in the Docker image.
    *   [ ] Use a multi-stage build to keep the final image size small.

5.  **Set Up Deployment Infrastructure:**
    *   [ ] Choose a cloud provider (e.g., AWS, GCP, Azure) and a hosting service (e.g., Heroku, AWS Elastic Beanstalk, Google Cloud Run).
    *   [ ] Implement a solution for storing and serving generated audio files (e.g., using a cloud storage service like Amazon S3 or Google Cloud Storage).
    *   [ ] Set up a CI/CD pipeline (e.g., using GitHub Actions) to automate testing and deployment.

## Phase 4: Frontend Development (Optional)

1.  **Build a User Interface:**
    *   [ ] Create a simple web interface that allows users to upload a drum sheet.
    *   [ ] Display the generated audio file for playback or download.
    *   [ ] Show the status of the parsing and audio generation process.
